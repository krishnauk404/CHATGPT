<!DOCTYPE html>
<html>
    <Head>
        <meta charset="utf-8">
        <title>My Portfolio</title>
        <link rel="stylesheet" href="style.css">
    </Head>
    <body>
        <h1>THIS IS CHATGPT</h1>
        <p class="p-welcome">
            <u1>
                ChatGPT, as part of the GPT-3.5 architecture, is made using a combination of data collection, pre-training, and fine-tuning processes. Here's a high-level overview of how ChatGPT is created:

            <li> Data Collection: The first step in creating ChatGPT is to gather a vast amount of text data from the internet. This data includes books, articles, websites, conversations, and more. The data is carefully curated and filtered to remove any sensitive or inappropriate content.</li>
            
           <li> Pre-Training: With the collected data, the model undergoes pre-training. During pre-training, the model learns to predict the next word in a sentence based on the patterns and context present in the data. It utilizes a deep neural network architecture with a large number of parameters to capture complex language patterns.
            
            <Li> Transformer Architecture: ChatGPT, like its predecessors, is built upon the transformer architecture. Transformers are a type of deep learning model designed to handle sequential data, making them particularly well-suited for natural language processing tasks. The transformer architecture introduced the self-attention mechanism, enabling the model to focus on relevant parts of the input text while processing it.
            
           <li> GPT Framework: ChatGPT is part of the "Generative Pre-trained Transformer" (GPT) family, which was pioneered by OpenAI. GPT models, including GPT-3.5, are designed to generate human-like text by utilizing the pre-training and fine-tuning process.
            
           <li> Fine-Tuning: After pre-training on a large corpus of data, the model is fine-tuned on specific tasks or datasets to make it more useful and controlled. Fine-tuning involves training the model on more specific and relevant data for a particular application, such as chat-based conversations.
            
           <li> Human-in-the-Loop: Throughout the process of creating and refining the model, human reviewers play an essential role in providing feedback and making judgments about model outputs. This iterative feedback loop helps improve the model's behavior and align it with desired ethical and safety guidelines.
            
            <li>It's worth noting that ChatGPT's training requires significant computational resources, including powerful GPUs and vast amounts of data. This process results in a language model that can generate coherent and contextually relevant responses based on the input it receives. However, it's essential to be aware that the model may still produce incorrect or inappropriate responses, as it is not capable of genuine understanding or common-sense reasoning. The continuous effort is put into refining and improving the model's capabilities while addressing potential limitations.
        </u1>
        </p>
    </body>
</html>